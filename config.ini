# Natural Language to First Order Logic
[PARSING]
# enable verbose conversion
VERBOSE = false
# WordNet language
LANGUAGE = eng
# lemmas triggering assignment rules
ASSIGN_RULES_LEMMAS = Be.v.01, Be.v.02, Be.v.05, Be
ASSIGN_RULES_POS = VBZ, VBP
# Axioms related words (Upcase)
AXIOMS_WORDS =
# Lemmatization activation
LEMMATIZATION = true


[AGENT]
# Waiting seconds before returning to idle state
WAIT_TIME = 20
# Operations logging
LOG_ACTIVE = True
# Ontology filename (.owl)
FILE_NAME = test.owl


[REASONING]
ACTIVE = true
# HERMIT, PELLET
REASONER = HERMIT
# SPARQL queries Prefixes
PREFIXES =  PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>, PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>,


# Selective inclusion/exclusion of Part-of-Speech
[POS]
INCLUDE_ACT_POS = true
INCLUDE_NOUNS_POS = true
INCLUDE_ADJ_POS = true
INCLUDE_PRP_POS = true
INCLUDE_ADV_POS = true
# Object adjective into noun correction
OBJ_JJ_TO_NOUN = true
# Separator
SEP = _


# POS Predicate labels encoding with WordNet synsets
[DISAMBIGUATION]
DIS_ACTIVE = false
DIS_VERB = VBZ, VBP, VB, VBD
DIS_NOUN = NN, NNS
DIS_ADJ = JJ
DIS_ADV = RB
DIS_EXCEPTIONS = equal, great, than, less
# GLOSS: doc2vect similarity with gloss
# EXAMPLES: best doc2vect similarity between examples (whether existing)
# BEST: best doc2vect similarity between GLOSS and EXAMPLES (whether existing)
# AVERAGE: average doc2vect similarity between glosses and examples (whether existing)
# COMBINED: doc2vect similarity between gloss+examples
DIS_METRIC_COMPARISON = EXAMPLES

# Common meaning for same lemmas in a session-context (DIS_ACTIVE = true)
[GROUNDED_MEANING_CONTEXT]
GMC_ACTIVE = false
GMC_POS = NN, NNP, NNS, VBZ, VBP, JJ


[LLM]
# Legenda interaction MODE:
# KG: LODO inference without OWL-to-NL response (logical forms)
# KGLLM: LODO inference with OWL-to-NL response
# LLM: only LLM question/response
# DUAL: (LLM+KGLLM)
MODE = KG
# Fol-to_NL Temperature
TEMP_FOL = 0.1
# Question Answering Temperature
TEMP_QA = 0.6
# Max new token number generation
MAX_NEW_TOKENS = 512
# Combination type for Multiple LoRA Adapters: cat (concatenation), linear (linear combination), svd (singular value decomposition)
COMB_TYPE = cat
# Combination weights (FOL, QA)
WEIGHTS = 0.7, 0.1
# Base model
BASE_MODEL = /home/fabio/llama/models/7B-chat
# Adapters names
ADAPTER_NAME1 = llama-sem_50ep
ADAPTER_NAME2 = llama-dolly_qa_100ep
# Adapters path
ADAPTER_PATH1 = /home/fabio/llama/models/finetuned/llama-fol2_50ep
ADAPTER_PATH2 = /home/fabio/llama/models/finetuned/llama-dolly_qa_100ep
# Forced response from LLM when unknown
FORCED_ANSWER_FROM_LLM = true
# Word replacer for blank, to force above response from LLM
REPLACER = whatever
