# Natural Language to First Order Logic
[PARSING]
# enable verbose conversion
VERBOSE = false
# WordNet language
LANGUAGE = eng
# lemmas triggering assignment rules
ASSIGN_RULES_LEMMAS = Be.v.01, Be.v.02, Be.v.05, Be
ASSIGN_RULES_POS = VBZ, VBP
# Axioms related words (Upcase)
AXIOMS_WORDS = WHEN


[AGENT]
# Waiting seconds before returning to idle state
WAIT_TIME = 20
# Operations logging
LOG_ACTIVE = True
# Ontology filename (.owl)
FILE_NAME = test.owl

# Selective inclusion/exclusion of Part-of-Speech
[POS]
INCLUDE_ACT_POS = false
INCLUDE_NOUNS_POS = false
INCLUDE_ADJ_POS = false
INCLUDE_PRP_POS = false
INCLUDE_ADV_POS = false
# Object adjective into noun correction (WARNING: false only with GEN_ADJ = false)
OBJ_JJ_TO_NOUN = false


# POS Predicate labels encoding with WordNet synsets
[DISAMBIGUATION]
DIS_ACTIVE = false
DIS_VERB = VBZ, VBP, VB, VBD
DIS_NOUN = NN, NNS
DIS_ADJ = JJ
DIS_ADV = RB
DIS_EXCEPTIONS = equal, great, than, less
# GLOSS: doc2vect similarity with gloss
# EXAMPLES: best doc2vect similarity between examples (whether existing)
# BEST: best doc2vect similarity between GLOSS and EXAMPLES (whether existing)
# AVERAGE: average doc2vect similarity between glosses and examples (whether existing)
# COMBINED: doc2vect similarity between gloss+examples
DIS_METRIC_COMPARISON = EXAMPLES

# Common meaning for same lemmas in a session-context (DIS_ACTIVE = true)
[GROUNDED_MEANING_CONTEXT]
GMC_ACTIVE = false
GMC_POS = NN, NNP, NNS, VBZ, VBP, JJ


# Question Answering
# LOC_PREPS: in case of where-question, they are all prepositions of possible candidates (possible responses to the question) will make usage
# TIME_PREPS: in case of when-question, they are all prepositions of possible candidates (possible responses to the question) will make usage
# COP_VERB: copula is-a for which (Obj1 is-a Obj2) => (Obj2 is-a Obj1)
# ROOT_TENSE_DEBT: all question ROOT tenses that after Question->Answer translations must be present also in the assertions
[QA]
LOC_PREPS = in, at
TIME_PREPS = in, on
COP_VERB = is, was, were
ROOT_TENSE_DEBT = did:VBD, does:VBZ
SHOW_REL = false


[LLM]
# LLM Interaction mode: KG (Abduction-Deduction with LODO OWL-to-NL response), LLM (only Query/Answer LLM), DUAL (KG+LLM)
MODE = AD
# Fol-to_NL Temperature
TEMP_FOL = 0.1
# Question Answering Temperature
TEMP_QA = 0.6
# Max new token number generation
MAX_NEW_TOKENS = 512
# Combination type for Multiple LoRA Adapters: cat (concatenation), linear (linear combination), svd (singular value decomposition)
COMB_TYPE = cat
# Combination weights (FOL, QA)
WEIGHTS = 0.7, 0.1
# Base model
BASE_MODEL = /home/fabio/llama/models/7B-chat
# Adapters names
ADAPTER_NAME1 = llama-sem_50ep
ADAPTER_NAME2 = llama-dolly_qa_100ep
# Adapters path
ADAPTER_PATH1 = /home/fabio/llama/models/finetuned/llama-fol2_50ep
ADAPTER_PATH2 = /home/fabio/llama/models/finetuned/llama-dolly_qa_100ep
# Prefix text for non-hot topic LLM response
PREFIX_LLM_RESP = Well...I'm not sure, but....
